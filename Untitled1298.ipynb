{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16369951-a672-4619-a4af-47231e17e144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: tables in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.10.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tables) (2.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tables) (24.2)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tables) (9.0.0)\n",
      "Requirement already satisfied: blosc2>=2.3.0 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tables) (3.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tables) (4.12.2)\n",
      "Requirement already satisfied: ndindex in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from blosc2>=2.3.0->tables) (1.10.0)\n",
      "Requirement already satisfied: msgpack in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from blosc2>=2.3.0->tables) (1.1.1)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from blosc2>=2.3.0->tables) (4.3.8)\n",
      "Requirement already satisfied: requests in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from blosc2>=2.3.0->tables) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->blosc2>=2.3.0->tables) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->blosc2>=2.3.0->tables) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->blosc2>=2.3.0->tables) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->blosc2>=2.3.0->tables) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas pyyaml tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b18bff1-4535-4b93-825f-d595a6896b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading datasets...\n",
      "[INFO] Combined shape: (44898, 5)\n",
      "[INFO] Columns: ['title', 'text', 'subject', 'date', 'label']\n",
      "[WRITE] Pickle (gzip) -> C:\\Users\\sagni\\Downloads\\News Sense\\news_dataset.pkl.gz\n",
      "[INFO] HDF5 min_itemsize: {'title': 350, 'text': 51858, 'subject': 79, 'date': 213}\n",
      "[WRITE] HDF5 (table, compressed) -> C:\\Users\\sagni\\Downloads\\News Sense\\news_dataset.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tables\\leaf.py:430: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  expected_mb = (expectedrows * rowsize) // MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] JSONL (streaming) -> C:\\Users\\sagni\\Downloads\\News Sense\\news_dataset.jsonl\n",
      "  - JSONL wrote chunk 1/3 (20000 rows)\n",
      "  - JSONL wrote chunk 2/3 (20000 rows)\n",
      "  - JSONL wrote chunk 3/3 (4898 rows)\n",
      "[WRITE] Meta YAML -> C:\\Users\\sagni\\Downloads\\News Sense\\news_dataset_meta.yaml\n",
      "[INFO] Writing YAML subset of 5000 rows (YAML_MAX_ROWS=5000)\n",
      "[WRITE] YAML -> C:\\Users\\sagni\\Downloads\\News Sense\\news_dataset.yaml\n",
      "\n",
      "[DONE] Outputs in: C:\\Users\\sagni\\Downloads\\News Sense\n",
      " - C:\\Users\\sagni\\Downloads\\News Sense\\news_dataset.pkl.gz (compressed pickle)\n",
      " - C:\\Users\\sagni\\Downloads\\News Sense\\news_dataset.h5 (HDF5; if present)\n",
      " - C:\\Users\\sagni\\Downloads\\News Sense\\news_dataset.jsonl (JSON Lines)\n",
      " - C:\\Users\\sagni\\Downloads\\News Sense\\news_dataset.yaml (YAML subset)\n",
      " - C:\\Users\\sagni\\Downloads\\News Sense\\news_dataset_meta.yaml (metadata YAML)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# NewsSense â€” FINAL Writers: PKL.GZ / H5 (min_itemsize) / JSONL / YAML\n",
    "# ==========================================================\n",
    "# Inputs:\n",
    "#   \"C:\\Users\\sagni\\Downloads\\News Sense\\archive\\News _dataset\\True.csv\"\n",
    "#   \"C:\\Users\\sagni\\Downloads\\News Sense\\archive\\News _dataset\\Fake.csv\"\n",
    "#\n",
    "# Outputs:\n",
    "#   C:\\Users\\sagni\\Downloads\\News Sense\n",
    "#     - news_dataset.pkl.gz\n",
    "#     - news_dataset.h5                  (with per-column min_itemsize)\n",
    "#     - news_dataset.jsonl               (streaming)\n",
    "#     - news_dataset.yaml                (subset; configurable)\n",
    "#     - news_dataset_meta.yaml           (schema + class balance; YAML-safe)\n",
    "# ==========================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Optional YAML ----------\n",
    "try:\n",
    "    import yaml\n",
    "    HAVE_YAML = True\n",
    "except Exception:\n",
    "    HAVE_YAML = False\n",
    "    print(\"[WARN] PyYAML not installed; YAML output will be skipped.\")\n",
    "\n",
    "# ---------- Paths (edit OUT_DIR if disk is tight) ----------\n",
    "TRUE_PATH = Path(r\"C:\\Users\\sagni\\Downloads\\News Sense\\archive\\News _dataset\\True.csv\")\n",
    "FAKE_PATH = Path(r\"C:\\Users\\sagni\\Downloads\\News Sense\\archive\\News _dataset\\Fake.csv\")\n",
    "OUT_DIR   = Path(r\"C:\\Users\\sagni\\Downloads\\News Sense\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE       = \"news_dataset\"\n",
    "PKL_PATH   = OUT_DIR / f\"{BASE}.pkl.gz\"\n",
    "H5_PATH    = OUT_DIR / f\"{BASE}.h5\"\n",
    "JSONL_PATH = OUT_DIR / f\"{BASE}.jsonl\"\n",
    "YAML_PATH  = OUT_DIR / f\"{BASE}.yaml\"\n",
    "META_YAML  = OUT_DIR / f\"{BASE}_meta.yaml\"\n",
    "\n",
    "# ---------- Writer config ----------\n",
    "JSONL_CHUNK_ROWS = 20_000\n",
    "YAML_MAX_ROWS    = 5_000        # set None to dump all rows (not recommended for huge files)\n",
    "H5_COMPLIB       = \"blosc:zstd\" # compression for HDF5 where supported\n",
    "H5_COMPLEVEL     = 5\n",
    "\n",
    "# ---------- Load ----------\n",
    "print(\"[INFO] Loading datasets...\")\n",
    "true_df = pd.read_csv(TRUE_PATH, low_memory=False)\n",
    "true_df[\"label\"] = 0   # 0 = True/Real\n",
    "fake_df = pd.read_csv(FAKE_PATH, low_memory=False)\n",
    "fake_df[\"label\"] = 1   # 1 = Fake\n",
    "\n",
    "df = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "print(\"[INFO] Combined shape:\", df.shape)\n",
    "print(\"[INFO] Columns:\", list(df.columns))\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def to_py(v):\n",
    "    \"\"\"Convert numpy/pandas scalars to plain Python; Ts/NaT -> str/None.\"\"\"\n",
    "    import pandas as pd\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, (np.integer,)):\n",
    "        return int(v)\n",
    "    if isinstance(v, (np.floating,)):\n",
    "        # preserve NaN as None\n",
    "        return None if np.isnan(v) else float(v)\n",
    "    if isinstance(v, (np.bool_,)):\n",
    "        return bool(v)\n",
    "    # pandas Timestamp/Timedelta/NA to string or None\n",
    "    if isinstance(v, (pd.Timestamp,)):\n",
    "        return v.isoformat()\n",
    "    if pd.isna(v):\n",
    "        return None\n",
    "    return v  # str or python native\n",
    "\n",
    "def compute_meta_yaml_safe(df_in: pd.DataFrame) -> dict:\n",
    "    cols = []\n",
    "    for c in df_in.columns:\n",
    "        series = df_in[c]\n",
    "        ex = None\n",
    "        # find first non-null example and coerce to python type\n",
    "        for val in series.head(100).tolist():\n",
    "            if pd.notna(val):\n",
    "                ex = to_py(val)\n",
    "                break\n",
    "        cols.append({\n",
    "            \"name\": str(c),\n",
    "            \"dtype\": str(series.dtype),\n",
    "            \"non_null\": int(series.notna().sum()),\n",
    "            \"nulls\": int(series.isna().sum()),\n",
    "            \"example\": ex if ex is None or isinstance(ex, (str, int, float, bool)) else str(ex),\n",
    "        })\n",
    "    counts = df_in[\"label\"].value_counts(dropna=False).to_dict() if \"label\" in df_in.columns else {}\n",
    "    counts_py = {str(k): int(v) for k, v in counts.items()}\n",
    "    return {\n",
    "        \"rows\": int(len(df_in)),\n",
    "        \"columns\": int(len(df_in.columns)),\n",
    "        \"label_counts\": counts_py,\n",
    "        \"columns_info\": cols\n",
    "    }\n",
    "\n",
    "def build_min_itemsize(df_in: pd.DataFrame, cap: int = 65500, headroom: int = 64) -> dict:\n",
    "    \"\"\"\n",
    "    Compute min_itemsize per object column for HDF5(table). Adds small headroom.\n",
    "    Cap protects against absurd lengths (PyTables practical limit ~65k).\n",
    "    \"\"\"\n",
    "    mi = {}\n",
    "    for c in df_in.select_dtypes(include=[\"object\"]).columns:\n",
    "        # robust max length without exploding memory\n",
    "        try:\n",
    "            max_len = int(df_in[c].astype(str).str.len().max() or 0)\n",
    "        except Exception:\n",
    "            max_len = 0\n",
    "        size = min(cap, max_len + headroom)\n",
    "        # Only set if there is actual text\n",
    "        if size > 0:\n",
    "            mi[c] = size\n",
    "    return mi\n",
    "\n",
    "def coerce_for_hdf(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Make HDF5-friendly dtypes: object->str, ints/floats normalized.\"\"\"\n",
    "    g = df_in.copy()\n",
    "    # ensure strings for object columns\n",
    "    for c in g.select_dtypes(include=[\"object\"]).columns:\n",
    "        g[c] = g[c].astype(str)\n",
    "    # numeric normalization\n",
    "    for c in g.columns:\n",
    "        s = g[c]\n",
    "        if pd.api.types.is_integer_dtype(s):\n",
    "            g[c] = s.astype(\"int64\", copy=False)\n",
    "        elif pd.api.types.is_float_dtype(s):\n",
    "            g[c] = s.astype(\"float64\", copy=False)\n",
    "        elif pd.api.types.is_bool_dtype(s):\n",
    "            g[c] = s.astype(bool, copy=False)\n",
    "    return g\n",
    "\n",
    "# ---------- 1) Compressed Pickle ----------\n",
    "try:\n",
    "    print(f\"[WRITE] Pickle (gzip) -> {PKL_PATH}\")\n",
    "    df.to_pickle(PKL_PATH, compression=\"gzip\")\n",
    "except OSError as e:\n",
    "    print(f\"[ERROR] Pickle write failed: {e}\")\n",
    "\n",
    "# ---------- 2) HDF5 with per-column min_itemsize ----------\n",
    "# Single write (fixed) or table with min_itemsize; we use table for queryability.\n",
    "try:\n",
    "    df_h5 = coerce_for_hdf(df)\n",
    "    min_itemsize = build_min_itemsize(df_h5, cap=65500, headroom=64)\n",
    "    print(\"[INFO] HDF5 min_itemsize:\", min_itemsize)\n",
    "\n",
    "    # remove prior file to avoid schema conflicts\n",
    "    if H5_PATH.exists():\n",
    "        try:\n",
    "            H5_PATH.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(f\"[WRITE] HDF5 (table, compressed) -> {H5_PATH}\")\n",
    "    df_h5.to_hdf(\n",
    "        H5_PATH,\n",
    "        key=\"news\",\n",
    "        mode=\"w\",\n",
    "        format=\"table\",           # appendable/queryable\n",
    "        complib=H5_COMPLIB,\n",
    "        complevel=H5_COMPLEVEL,\n",
    "        min_itemsize=min_itemsize # <-- critical fix for long strings\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] HDF5 write skipped/failed: {e}\\n       Hint: ensure 64â€‘bit Python, `pip install tables`, and ample free disk.\")\n",
    "\n",
    "# ---------- 3) JSONL (streaming, robust) ----------\n",
    "try:\n",
    "    print(f\"[WRITE] JSONL (streaming) -> {JSONL_PATH}\")\n",
    "    total = len(df)\n",
    "    n_chunks = math.ceil(total / JSONL_CHUNK_ROWS)\n",
    "    with open(JSONL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(n_chunks):\n",
    "            s = i * JSONL_CHUNK_ROWS\n",
    "            e = min((i + 1) * JSONL_CHUNK_ROWS, total)\n",
    "            chunk = df.iloc[s:e].where(pd.notna(df.iloc[s:e]), None)\n",
    "            for rec in chunk.to_dict(orient=\"records\"):\n",
    "                # normalize to python types\n",
    "                for k, v in list(rec.items()):\n",
    "                    if isinstance(v, (np.integer,)):\n",
    "                        rec[k] = int(v)\n",
    "                    elif isinstance(v, (np.floating,)):\n",
    "                        rec[k] = None if np.isnan(v) else float(v)\n",
    "                    elif isinstance(v, (np.bool_,)):\n",
    "                        rec[k] = bool(v)\n",
    "                f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            print(f\"  - JSONL wrote chunk {i+1}/{n_chunks} ({e - s} rows)\")\n",
    "except OSError as e:\n",
    "    print(f\"[ERROR] JSONL write failed: {e}\\n       Tip: change OUT_DIR to a drive with more free space.\")\n",
    "\n",
    "# ---------- 4) YAML (meta + subset) ----------\n",
    "if HAVE_YAML:\n",
    "    try:\n",
    "        meta = compute_meta_yaml_safe(df)\n",
    "        print(f\"[WRITE] Meta YAML -> {META_YAML}\")\n",
    "        with open(META_YAML, \"w\", encoding=\"utf-8\") as f:\n",
    "            yaml.safe_dump(meta, f, allow_unicode=True, sort_keys=False)\n",
    "\n",
    "        if YAML_MAX_ROWS is None or YAML_MAX_ROWS >= len(df):\n",
    "            subset = df\n",
    "            print(f\"[INFO] Writing full YAML with {len(df)} rows (may be large)\")\n",
    "        else:\n",
    "            subset = df.iloc[:YAML_MAX_ROWS]\n",
    "            print(f\"[INFO] Writing YAML subset of {len(subset)} rows (YAML_MAX_ROWS={YAML_MAX_ROWS})\")\n",
    "\n",
    "        # convert to python-native types\n",
    "        subset_py = subset.where(pd.notna(subset), None).to_dict(orient=\"records\")\n",
    "        for rec in subset_py:\n",
    "            for k, v in list(rec.items()):\n",
    "                rec[k] = to_py(v)\n",
    "                if not isinstance(rec[k], (str, int, float, bool)) and rec[k] is not None:\n",
    "                    rec[k] = str(rec[k])\n",
    "\n",
    "        print(f\"[WRITE] YAML -> {YAML_PATH}\")\n",
    "        with open(YAML_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            yaml.safe_dump(subset_py, f, allow_unicode=True, sort_keys=False)\n",
    "\n",
    "    except OSError as e:\n",
    "        print(f\"[WARN] YAML write failed: {e}\\n       Tip: reduce YAML_MAX_ROWS or move OUT_DIR to a larger drive.\")\n",
    "else:\n",
    "    print(\"[INFO] Skipping YAML (PyYAML not installed).\")\n",
    "\n",
    "# ---------- Done ----------\n",
    "print(\"\\n[DONE] Outputs in:\", OUT_DIR)\n",
    "print(\" -\", PKL_PATH, \"(compressed pickle)\")\n",
    "print(\" -\", H5_PATH, \"(HDF5; if present)\")\n",
    "print(\" -\", JSONL_PATH, \"(JSON Lines)\")\n",
    "if HAVE_YAML:\n",
    "    print(\" -\", YAML_PATH, \"(YAML subset)\")\n",
    "    print(\" -\", META_YAML, \"(metadata YAML)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173972cd-6ead-4a8b-91c8-d4d157334e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
